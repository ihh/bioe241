\documentclass{beamer}

\input{../defs.tex}

% workaround for beamer bug
\providecommand\thispdfpagelabel[1]{}  % workaround

% presentation
\mode<presentation>
{
  \usetheme{Warsaw}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage[english]{babel}
% or whatever

\usepackage[latin1]{inputenc}
% or whatever

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.


\title[Indels] % (optional, use only with long paper titles)
{Insertions and deletions}

\subtitle
{Probabilistic models of indel evolution} % (optional)

\author% [Holmes] (optional, use only with lots of authors)
{I.~Holmes} % \inst{1} \and S.~Another\inst{2}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[University of California, Berkeley] % (optional, but mostly needed)
{
%  \inst{1}%
  Department of Bioengineering\\
  University of California, Berkeley}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date%[Short Occasion] % (optional)
{Spring semester}

\subject{Talks}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

%\beamerdefaultoverlayspecification{<+->}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}


\section{Mechanisms of sequence mutation}

\begin{frame}{}

\itemb
\item Point substitution: the canonical models
\item Context-dependent and multiple-nucleotide substitutions
 \inone{e.g. CpG $\to$ 5-methyl-cytosine (5mC) $\to$ C-deamination $\to$ TpG}
\item Insertions and deletions; mechanisms
 \itemb
 \item DNA foldback, cruciforms, etc; polymerase stutter; microsatellites
 \item Transposition (DNA cut'n'paste; retrotransposition; rolling-circle); horizontal transfer
 \iteme
\item Duplications (local, nonlocal); inversions; whole-chromosome and -genome duplications, polyploidy
\item Rearrangement
\item Recombination; gene conversion
\iteme

\end{frame}

\section{The ``links model'' and string transducers}

\begin{frame}{}

\itemb
\item Motivation: how to handle gaps? Gaps as a fifth nucleotide in standard point substitution model: advantages (simplicity), drawbacks (irreversibility, ``ghost'' bases)
\item Whole-sequence models: state space $\Omega^\ast$. Rate grammar notation: write mutation rules
 \itemb
 \item Point substitution with rate matrix ${\bf Q}$: rule is $AxB\ \to\ AyB:\ Q_{xy}$ where $A,B \in \Omega^\ast$ and $x,y \in \Omega$
 \item Insertion: rule can be written $AC\ \to\ ABC$, etc.
 \iteme
\item How to calculate conditional probabilities (matrix exponential) or multiple alignment likelihoods?
\inone{Infinite-dimensional (albeit sparse) rate matrix; total mutation rate scales with sequence length.}
\item Under point sub. model, likelihood for whole alignment factorizes into product of column likelihoods.
Each column is independently evolving zone. Extension to indel models uses same idea of independent zones.
\iteme
\end{frame}

\begin{frame}{}
\itemb
\item Thorne, Kishino, Felsenstein 1991. {\bf Links model.}
 \itemb
 \item First consider the following rate grammar: \\
\begin{tabular}{l|rll|l}
Event & \multicolumn{3}{c|}{Rule} & Rate \\
\hline
Substitution & $AxB$ & $\to$ & $AyB$ & $Q_{xy}$ \\
Insertion & $AB$ & $\to$ & $AyB$ & $\lambda \pi_y$ \\
Deletion & $AxB$ & $\to$ & $AB$ & $\mu$
\end{tabular}
 \item Here $A,B \in \Omega^\ast$, $x,y \in \Omega$, ${\bf Q}$ is a reversible Markov process on $\Omega$ with equilibrium $\pi$,
$\lambda$ is a point insertion rate and $\mu$ is a point deletion rate
 \iteme
\iteme
\end{frame}

\begin{frame}{}
\itemb
\item Reversibility and equilibrium of the model
 \itemb
 \item Number of links in sequence evolves independently from actual nucleotides themselves
 \item Let $n$ be sequence length (i.e. number of mortal links)
  \itemb
  \item $n$ evolves according to a classical ``linear birth-death process with constant immigration'' (in fact immigration rate = birth rate)
  \item Total insertion rate (i.e. rate of $n \to n+1$) is $\lambda (n+1)$
  \item Total deletion rate (i.e. rate of $n \to n-1$) is $\mu n$
  \item If reversibility holds, then $P(n) \times \mbox{Rate}(n \to n+1) = P(n+1) \times \mbox{Rate}(n+1 \to n)$ where $P(n)$ is equilibrium length distribution
  \item Thus $P(n) \lambda(n+1) = P(n+1) \mu(n+1)$ so that $P(n) = \kappa^n (1-\kappa)$ where $\kappa = \frac{\lambda}{\mu}$ i.e. geometric
  \item NB for normalisation, $\kappa < 1 \Rightarrow \lambda < \mu$
  \item Expected sequence length at equilbrium is $\frac{\kappa}{1-\kappa}$. As $\lambda \to \mu$, equilibrium sequence length $\to \infty$
  \iteme
 \iteme
\iteme
\end{frame}

\begin{frame}{}
\itemb
\item Equation of state for $n(t)$. Here $P_t(n') = P(n(t)=n')$
\[
\derivop{t} P_t(n) = \lambda n P_t(n-1) + \mu (n+1) P_t(n+1) - (\lambda n + \mu (n+1)) P_t(n)
\]
Same as equation for immortal zone fates (see below)
\item Clearly individual nucleotides are distributed according to $\pi$ at equilibrium
(since newly-inserted nucleotides are also sampled from this distribution).
So equilibrium probability distribution over sequences $X$ is
\[
P(X) = \kappa^{|X|} (1-\kappa) \prod_{i=1}^{|X|} \pi_{X_i}
\]
NB this is also the likelihood for generating $X$ from a single-state HMM with self-loop transition probability $\kappa$ and emit vector $\pi$.
\\
The mean sequence length is $\kappa/(1-\kappa)$. % and this is also the ``tipping point'' above which the total deletion rate exceeds the total insertion rate.
\iteme
\end{frame}

\begin{frame}{}
\itemb
\item TK\&F % introduced the following construction to help analyse this model
 (following Bishop \& Thompson, 1986)
 \itemb
 \item A biological sequence is a chain of {\em links}: one ``immortal link'' followed by zero or more normal or ``mortal links''
 \item Mortal and immortal links spawn new ``child links'' to their right (rate $\lambda$). Mortal links can also die (rate $\mu$).
 \item Each mortal link corresponds to an observed, independently evolving nucleotide (or amino acid).
 \item The immortal link is invisible.
%(Without the immortal link, if the sequence ever reached zero length it would get irreversibly stuck.
%This might or might not be realistic, but one goal of Thorne {\em et al} was a reversible model.)
 \iteme
\iteme
\end{frame}

\begin{frame}{}
\small
\framebox{\parbox{\textwidth}{\em ``The insertion-deletion process is framed in terms
of a birth-death process of these links.  Each link 
evolves independently from all other link; a  birth 
or death of one  link does not affect the probability 
of a birth or death of any other link. Both types of 
links can be associated with births. The birth  rate per normal link
is equal to the birth rate per
immortal link ($\lambda$). A newborn link is always a normal 
link. We adopt the convention that it appears immediately to the right of its parent link.
Accompanying  the birth of a normal  link  is  the  birth of a 
DNA base immediately to the left of the newborn 
link.  The probabilities that the newborn DNA base 
will be A, G, T, or C are $\pi_A$, $\pi_G$, $\pi_T$ and $\pi_C$, respectively.  Normal  links are subject to death ($\mu$ is 
the death  rate per normal link) but immortal links 
are not.''}}
\normalsize
\end{frame}

\begin{frame}{}

Solving for conditional probabilities $P(\mbox{descendant}|\mbox{ancestor})$ % in TKF91
 \itemb
 \item Consider an ancestral sequence with $n$ mortal links (and one immortal link)
 \item Each ancestral link defines a zone that evolves independently from all other zones
 \item Zones corresponding to mortal ancestral links can acquire new links, lose links (including the original mortal link) and even die off
(i.e. lose all its links, and become inert)
  \itemb
  \item NB this implies the process w.r.t. zones is irreversible. Apparent paradox arises because we've introduced new information
in the form of the zone (alignment) co-ordinates, and our original model did not promise to be reversible with respect to zones (alignments).
  \iteme
 \iteme
\end{frame}

\begin{frame}{}
\itemb
  \item A specific consequence of fixing the zone co-ordinates is that the alignment
\begin{tabular}{l} {\tt X-} \\ {\tt -X} \end{tabular}
is distinct from the alignment
\begin{tabular}{l} {\tt -X} \\ {\tt X-} \end{tabular},
since the former implies a deletion followed by an insertion in the same zone,
whereas the latter does not imply any chronological ordering on the insertion \& deletion events, since they occurred in different zones.
(This asymmetry relates to the apparent contradiction to reversibility mentioned above. Both can, in fact, be ``fixed'' by swapping the order of such alignment columns when reversing the arrow of time.)
 \item The zone corresponding to the immortal link can acquire and lose new links, but never dies off
\iteme
\end{frame}

\begin{frame}{}
\itemb
 \item Differential equations for zone fates. Suppose time $t$ has elapsed since ancestral sequence observed. Let
  \itemb
  \item $p_n(t)$ be the probability that $n$ mortal links are descended from a mortal link {\bf and that one of them is the original};
  \item $q_n(t)$ be the probability that $n$ mortal links are descended from a mortal link {\bf and that the original died};
  \item $r_n(t)$ be the probability that $n$ mortal links are descended from an immortal link.
  \iteme
\iteme
\end{frame}

\begin{frame}{}
Differential equations:
\begin{eqnarray*}
\derivop{t} p_n & = & \lambda (n-1) p_{n-1} + \mu n p_{n+1} - (\lambda + \mu) n p_n \\
\derivop{t} q_n & = & \lambda (n-1) q_{n-1} + \mu (n+1) q_{n+1} + \mu p_{n+1} - (\lambda + \mu) n q_n \\
\derivop{t} r_n & = & \lambda n r_{n-1} + \mu (n+1) r_{n+1} - (\lambda (n+1) + \mu n) r_n
\end{eqnarray*}
Boundary conditions:
\begin{eqnarray*}
p_n(0) & = & \delta (n = 1) \\
q_n(0) & = & 0 \\
r_n(0) & = & \delta (n = 0)
\end{eqnarray*}
\end{frame}

\begin{frame}{}
Solutions:
\begin{eqnarray*}
p_n(t) & = & \alpha \beta^{n-1} (1-\beta) \\
q_0(t) & = & (1-\alpha) (1-\gamma) \\
q_n(t) & = & (1-\alpha) \gamma \beta^{n-1} (1-\beta) \quad \mbox{for $n>0$} \\
r_n(t) & = & \beta^n (1-\beta)
\end{eqnarray*}
where
\begin{eqnarray*}
\alpha(t) & = & \exp (-\mu t) \\
\beta(t) & = & \frac{1 - \alpha(t) \exp(\lambda t)}{\kappa^{-1} - \alpha(t) \exp(\lambda t)} \\
\gamma(t) & = & 1 - \frac{\beta(t)}{\kappa(1-\alpha(t))}
\end{eqnarray*}
\end{frame}

\begin{frame}{}
\itemb
 \item Thorne {\em et al} quote these results without derivation (they were obtained by comparison with formulae for similar generic birth-death processes).
They can readily be verified.
 \item Metzler argument: if $n$ is the no. of surviving links at time $t$, then $P(n \geq k+1|n \geq k)$ must be independent of $k$,
since $n \geq k$ implies that there has been an available insertion site for time $t$
  \inone{Holmes used this to derive numerical estimates of posterior expectations for number of indels}
 \item Feller's generating function approach can be used to solve for $r_n(t)$ and guess forms of $p_n,q_n$
  \inone{This also yields posterior summary statistics: Minin {\em et al}, \url{http://arxiv.org/abs/1009.0893}}
 \item Karlin and McGregor analysed birth-death processes in detail,
and found a series of orthogonal polynomials associated with transition probabilities of the process
\iteme
\end{frame}

\begin{frame}{}
{\bf Linear birth-death process with birth rate $\lambda$, death rate $\mu$ and (constant) immigration rate $\lambda$}

 Introduce generating function $G(s,t) = \sum_{n=0}^\infty s^n r_n(t)$.
The $r_n$ are recoverable as $r_n(t) = \left. \pderivn{G}{s}{n} \right|_{s=0}$

 Let $D = \pderivop{s}$ and use the following operator table: % to build a p.d.e.:
\begin{tabular}{|rcr|l|}
\hline
\multicolumn{3}{|r|}{Operator $L$}  & Coefficient of $s^n$ in $LG$ \\
\hline
& & 1 & $r_n$ \\
$s D s$ & $=$ & $s \left( 1 + s D \right)$ & $n r_{n-1}$ \\
& & $D$ & $(n+1) r_{n+1}$ \\
$D s$ & $=$ & $1 + s D$ & $(n+1) r_n$ \\
& & $s D$ & $n r_n$ \\
\hline
\end{tabular}
\small
\begin{eqnarray*}
\pderiv{G}{t} & = & \left[ \lambda s \left( 1 + s D \right) + \mu D - \lambda \left( 1 + s D \right) - \mu s D \right] G \\
& = & \lambda(s-1) G + (\lambda s - \mu)(s-1) \pderiv{G}{s}
\end{eqnarray*}
\normalsize
\end{frame}

\begin{frame}{}
\begin{eqnarray*}
\pderiv{G}{t} & = & \left[ \lambda s \left( 1 + s D \right) + \mu D - \lambda \left( 1 + s D \right) - \mu s D \right] G \\
& = & \lambda(s-1) G + (\lambda s - \mu)(s-1) \pderiv{G}{s}
\end{eqnarray*}
Can rewrite this as
\[
\frac{1}{\lambda(s-1)} \pderiv{G}{t} + (\mu/\lambda - s) \pderiv{G}{s} = G
\]
Boundary condition is $G(s,0) = 1$.

\end{frame}

\begin{frame}{}
Use method of characteristics to rewrite this p.d.e. as o.d.e.'s.

Suppose $s=s(u)$ and $t=t(u)$. Then
$\displaystyle
\deriv{G}{u} = \pderiv{G}{t} \deriv{t}{u} + \pderiv{G}{s} \deriv{s}{u}
$
which looks like our p.d.e. if
\begin{eqnarray*}
\deriv{t}{u} & = & \frac{1}{\lambda (s-1)} \\
\deriv{s}{u} & = & \mu/\lambda - s \\
\deriv{G}{u} & = & G
\end{eqnarray*}
The general solution for $G$ is
\[
G(s,t) = g(v) e^u
\]
where $g(\cdot)$ is any function and $v$ is constant along a characteristic, so $dv/du=0$.
Solving the o.d.e. for $s(u)$ (e.g. using an integrating factor) we obtain
$s = e^{-u} + \mu / \lambda$.
\end{frame}

\begin{frame}{}
Furthermore, on a characteristic curve, the following is true
\[
\deriv{t}{s} = \frac{dt/du}{ds/du} = \frac{1}{\lambda(s-1)(\mu/\lambda - s)}
\]
and hence
\[
\log \left| \frac{s - \mu / \lambda}{s - 1} \right| + (\mu - \lambda) t = \mbox{const.}
\]
The general solution for $G$ can thus be written
\[
G(s,t) = g \left( \frac{s - \mu / \lambda}{s - 1} e^{(\mu - \lambda) t} \right) (s - \mu / \lambda)^{-1}
\]
where $g(\cdot)$ is an arbitrary function, to be determined by the boundary condition -- which is $G(s,0)=1$, so
\[
g \left( \frac{s - \mu / \lambda}{s - 1} \right) = s - \mu / \lambda
\]
\end{frame}

\begin{frame}{}
Boundary condition $G(s,0)=1$ leads to
\begin{eqnarray*}
g \left( \frac{s - \mu / \lambda}{s - 1} \right) & = & s - \mu / \lambda \\
g(v) & = & (\mu / \lambda - 1) \left( \frac{1}{1 - v} - 1 \right) \\
G(s,t) & = & \frac{\mu / \lambda - 1}{\mu / \lambda - e^{(\lambda - \mu)t} - s \left( 1 - e^{(\lambda - \mu)t} \right)} \\
r_n(t) & = & \left. \pderivn{G}{s}{n} \right|_{s=0} \\
& = & \left. \frac{(\mu / \lambda - 1) \left( 1 - e^{(\lambda - \mu)t} \right)^n}{\left( \mu / \lambda - e^{(\lambda - \mu)t} + s \left( e^{(\lambda - \mu)t} - 1 \right) \right)^{n+1}} \right|_{s=0} \\
& = & \left( 1 - \beta(t) \right) \beta(t)^n
\end{eqnarray*}
as expected.
\end{frame}


\begin{frame}{}

Integrating factors.
Consider the equation
\[
y' + P(x)y = Q(x)
\]
Multiply by integrating factor $M(x)$
to yield
\[
M(x)y' + P(x)M(x)y = Q(x)M(x)
\]
If we choose $M(x)$ such that $M'(x) = M(x)P(x)$, then
\[
(M(x)y)' = Q(x)M(x)
\]

Equivalently $M'/M = P$ and so
\begin{eqnarray*}
M(x) & = & \exp \left[ \int P(x) dx \right] \\
y(x) & = & \frac{\int Q(x) M(x) dx + C}{M(x)}
\end{eqnarray*}
where $C$ is a constant of integration.

\end{frame}

\begin{frame}{}
TKF91 as a transducer and a Pair HMM
 \itemb
 \item Transducer: stochastic finite state machine that ``eats'' input symbols and ``emits'' output symbols,
representing the action of a finite time interval $t$ (ancestor=input, descendant=output)
  \itemb
  \item States {\tt START, INSERT, WAIT, MATCH, DELETE, END}
   \inone{{\tt WAIT} is a null state introduced for later convenience; it means ``wait for input symbol''}
  \item A transducer is similar to a Pair HMM, but normalised differently
   \inone{Forward likelihood is conditional $P(\mbox{des}|\mbox{anc})$ rather than joint $P(\mbox{anc},\mbox{des})$}
  \item Emission probabilities $\exp({\bf Q}t)_{xy}$ ({\tt MATCH} state), $\pi_y$ ({\tt INSERT} state), $1$ ({\tt DELETE} state)
  \iteme
 \iteme
\end{frame}

\begin{frame}{}
Transition matrix
\begin{tabular}{r|cccccc}
From/To & {\tt S} & {\tt I} & {\tt W} & {\tt M} & {\tt D} & {\tt E} \\
\hline
{\tt S} & . & $\beta$ & $1-\beta$ & . & . & . \\
{\tt I} & . & $\beta$ & $1-\beta$ & . & . & . \\
{\tt W} & . & . & . & $\alpha$ & $1-\alpha$ & 1 \\
{\tt M} & . & $\beta$ & $1-\beta$ & . & . & . \\
{\tt D} & . & $\gamma$ & $1-\gamma$ & . & . & . \\
{\tt E} & . & . & . & . & . & .
\end{tabular}
(dots represent zeroes)
\end{frame}

\begin{frame}{}
Can obtain joint Pair HMM for likelihood $P(\mbox{anc},\mbox{des})$ by ``left-multiplying'' transducer by single-state HMM for ancestor
  \itemb
  \item Ancestor states {\tt S,I,E}; transition matrix
\begin{tabular}{r|cccc}
From/To & {\tt S} & {\tt I} & {\tt E} \\
\hline
{\tt S} & . & $\kappa$ & $1-\kappa$ \\
{\tt I} & . & $\kappa$ & $1-\kappa$ \\
{\tt E} & . & . & .
\end{tabular}
  \item Joint anc-des states {\tt SS, SI, SW, IM, ID, II, IW, EE}
  \item Emission probabilities $\pi_x \exp({\bf Q}t)_{xy}$ ({\tt IM} state), $\pi_y$ ({\tt SI,II} states), $\pi_x$ ({\tt ID} state)
 \iteme
\end{frame}

\begin{frame}{}

Now consider two TKF91 transducers in series:

\vspace{\baselineskip}

\centerline{Ancestor
 $\stackrel{T(t)}{\longrightarrow}$ Intermediate
 $\stackrel{T(t)}{\longrightarrow}$ Descendant}

\vspace{\baselineskip}

Can we marginalize the intermediate sequence, and write this as a single transducer?

\vspace{\baselineskip}

\centerline{Ancestor
 $\stackrel{T(t)^2}{\longrightarrow}$ Descendant}

\vspace{\baselineskip}
For TKF91, we know the transducer models the finite-time
transition probabilities of a stochastic process,
so $T(t)^2 = T(2t)$.
However we can also think of the state space of the two coupled TKF91 machines
as a single ensemble transducer.

\end{frame}

\begin{frame}{}
Transition matrix
\begin{tabular}{r|ccccccccc}
From/To & {\tt SS} & {\tt SI} & {\tt SW} & {\tt IM} & {\tt ID} & {\tt II} & {\tt IW} & {\tt EE} \\
\hline
{\tt SS} & . & $\beta$ & $1-\beta$ & . & . & . & . \\
{\tt SI} & . & $\beta$ & $1-\beta$ & . & . & . & . & . \\
{\tt SW} & . & . & . & $\kappa\alpha$ & $\kappa(1-\alpha)$ & . & . & $1-\kappa$ \\
{\tt IM} & . & . & . & . & . & $\beta$ & $1-\beta$ & . \\
{\tt ID} & . & . & . & . & . & $\gamma$ & $1-\gamma$ & . \\
{\tt II} & . & . & . & . & . & $\beta$ & $1-\beta$ & . \\
{\tt IW} & . & . & . & $\kappa\alpha$ & $\kappa(1-\alpha)$ & . & . & $1-\kappa$ \\
{\tt EE} & . & . & . & . & . & . & . & .
\end{tabular}
\end{frame}

\begin{frame}{}
 Here, in constructing transitions for the expanded transducer, we have used the general rule:
{\em``Update the last transducer that is not in a {\tt WAIT} state. If this transducer emits an output symbol,
 then update any {\tt WAIT}-ing transducers that receive a symbol on their input,
 repeating this last step recursively until no more transducers have input symbols to process.''}

All transitions update one transducer only, except those from \{{\tt SW,IW}\} to \{{\tt IM,ID,EE}

Note redundancy: can eliminate {\tt SW} and {\tt IW}, collapse {\tt SI} and {\tt II} together...
in fact, for this model [TKF91], can collapse DP right down to one variable; see e.g. \Miklos, Song {\em et al}.
\end{frame}

\begin{frame}{}
\itemb
\item We just described transducer {\em composition}
 \itemb
 \item Output of transducer $A$ connected to input of $B$
 \item Analogous to matrix multiplication $AB$
 \iteme
\item Also useful to consider transducer {\em intersection}
 \itemb
 \item Input duplicated, fed to inputs of $A$ and $B$
 \item Analogous to pointwise (Hadamard) product, $A \circ B$
 \iteme
\item Generators, recognizers
 \inone{Unit generator $\Delta(S)$ and recognizer $\nabla(S)$}
\item Felsenstein pruning: Westesson {\em et al}, \url{http://arxiv.org/abs/1103.4347}
\iteme
\small
\centerline{ \url{http://en.wikipedia.org/wiki/Finite_state_transducer} }
\normalsize
\end{frame}

% Transducer defs
\newcommand\nullmodel{{\cal N}}
\newcommand\substitute{{\cal S}}
\newcommand\tkf{{\cal B}}
\newcommand\tkfroot{R}
\newcommand\formaldefs{{\bf Formal definitions: }}
\newcommand\charat[2]{\mbox{symbol}(#1,#2)}
\newcommand\gappedalphabet[1]{(\Omega_{#1} \cup \{\epsilon\})}
\newcommand\gapsquared{\gappedalphabet{}^2}
\newcommand\gappedpair[2]{\gappedalphabet{#1} \times \gappedalphabet{#2}}
\newcommand\wtrans[4]{#1(#2 : [#3] : #4)}
\newcommand\transequiv{\equiv}
\newcommand\compose{}
\newcommand\identity{{\cal I}}
\newcommand\fork{\circ}
\newcommand\idfork{\Upsilon}
\newcommand\forkn[1]{\idfork(#1)}
\newcommand\forkfun[2]{\forkn{#1, #2}}
\newcommand\generate{\Delta}
\newcommand\recognize{\nabla}
\newcommand\States{\Phi}
\newcommand\statesof[1]{\States_{#1}}
\newcommand\Transitions{\tau}
\newcommand\transitionsof[1]{\Transitions_{#1}}
\newcommand\startstate{\phi_S}
\newcommand\laststate{\phi_E}
\newcommand\startstateof[1]{\phi_{S;#1}}
\newcommand\laststateof[1]{\phi_{E;#1}}
\newcommand\weight{{\cal W}}
\newcommand\transweightfun{\weight_{\mbox{\tiny trans}}}
\newcommand\emitweightfun{\weight_{\mbox{\tiny emit}}}
\newcommand\sumoverpaths[1]{\transweightfun(\{\pi_{#1}\})}
\newcommand\statesoftype[1]{\States_{#1}}
\newcommand\statetype{\mbox{\tiny type}}
\newcommand\dup[1]{\left( \begin{array}{l} #1 \\ #1 \end{array} \right)}
\newcommand\seqlen[1]{\mbox{len}(#1)}
\newcommand\projection{{\cal P}}

\begin{frame}{}

{\em The letter transducer} is a tuple $T = (\Omega_I, \Omega_O, \States, \startstate, \laststate, \Transitions, \weight)$
where
\itemb
\item $\Omega_I$ is an input alphabet,
\item $\Omega_O$ is an output alphabet,
\item $\States$ is a set of states,
\item $\startstate \in \States$ is the start state,
\item $\laststate \in \States$ is the end state,
\item $\Transitions \subseteq \States \times \gappedalphabet{I} \times \gappedalphabet{O} \times \States$ is the transition relation,
\item $\weight:\Transitions \to [0,\infty)$ is the transition weight function.
\iteme
\end{frame}

\begin{frame}{}

{\em Transition paths:}
The transitions in $\Transitions$ correspond to the edges of a labeled multidigraph over states in $\States$.
Let $\Pi \subset \Transitions^\ast$ be the set of all labeled transition paths from $\startstate$ to $\laststate$.

{\em Transduction weight:}
For a transition path $\pi$ from $\startstate$ to $\laststate$,
define the path weight $\weight(\pi)$ and
(for sequences $x \in \Omega_I^\ast, y \in \Omega_O^\ast$)
the transduction weight $\wtrans{\weight}{x}{T}{y}$

\begin{eqnarray*}
\weight(\pi) & = & \prod_{\Transitions \in \pi} \weight(\Transitions) \\
\wtrans{\weight}{x}{T}{y} & = & \sum_{\pi \in \Pi, S_I(\pi)=x, S_O(\pi)=y} \weight(\pi)
\end{eqnarray*}
where $S_I$ and $S_O$ denote the input and output labelings of a path.
\end{frame}

\begin{frame}{}

{\em Equivalence:}
If for transducers $T,T'$ it is true that $\wtrans{\weight}{x}{T}{y}=\wtrans{\weight'}{x}{T'}{y}\ \forall x,y$ then the transducers are equivalent in weight, $T \transequiv T'$.
\end{frame}

\begin{frame}{}

{\em Types of state and transition:}
If there exists a function,
\itemb
\item $\statetype:\States \to {\cal T}$
\iteme
mapping states to types in the set
\[
{\cal T} = \{S,M,D,I,N,W,E\}
\]
and, furthermore, we can find functions
\itemb
\item $\transweightfun: \States^2 \to [0,\infty)$
\item $\emitweightfun: \gappedpair{I}{O} \times \States \to [0,\infty)$
\iteme
such that the labeled transition weights separate into transition and emission factors (see next slide),
then the transducer is in {\em (weak) normal form}.

If, additionally, there are no null states ($\statesoftype{N} = \emptyset$),
then the transducer is in {\em strict normal form}.

\end{frame}

\begin{frame}

\small
\begin{eqnarray*}
\States_U & = & \{ \phi: \phi\in\States, \statetype(\phi) \in U \subseteq {\cal T} \} \\
\statesoftype{S} & = & \{ \startstate \} \\
\statesoftype{E} & = & \{ \laststate \} \\
\States & \equiv & \statesoftype{SMDINWE} \\   
\Transitions_M & \subseteq & \statesoftype{W} \times \Omega_I \times \Omega_O \times \statesoftype{M} \\
\Transitions_D & \subseteq & \statesoftype{W} \times \Omega_I \times \{\epsilon\} \times \statesoftype{D} \\
\Transitions_I & \subseteq & \statesoftype{SMDIN} \times \{\epsilon\} \times \Omega_O \times \statesoftype{I} \\
\Transitions_N & \subseteq & \statesoftype{SMDIN} \times \{\epsilon\} \times \{\epsilon\} \times \statesoftype{N} \\
\Transitions_W & \subseteq & \statesoftype{SMDIN} \times \{\epsilon\} \times \{\epsilon\} \times \statesoftype{W} \\
\Transitions_E & \subseteq & \statesoftype{W} \times \{\epsilon\} \times \{\epsilon\} \times \statesoftype{E} \\
\Transitions & = & \Transitions_M \cup \Transitions_D \cup \Transitions_I \cup \Transitions_N \cup \Transitions_W \cup \Transitions_E \\
\weight(\phi_{\mbox{src}},\omega_{\mbox{in}},\omega_{\mbox{out}},\phi_{\mbox{dest}}) & \equiv & \transweightfun(\phi_{\mbox{src}},\phi_{\mbox{dest}}) \emitweightfun(\omega_{\mbox{in}},\omega_{\mbox{out}},\phi_{\mbox{dest}})
\end{eqnarray*}
\normalsize

\end{frame}

\begin{frame}{}

{\em Interpretation:}
A normal-form transducer can be thought of as associating inputs and outputs with states, rather than transitions.
(Thus, it is like a Moore machine.)
The state types are
 start ($S$) and end ($E$);
 wait ($W$), in which the transducer waits for input;
 match ($M$) and delete ($D$), which process input symbols;
 insert ($I$), which writes additional output symbols; and
 null ($N$), which has no associated input or output.
All transitions also fall into one of these types, via the destination states;
thus, $\Transitions_M$ is the set of transitions ending in a match state, etc.
The transition weight ($\weight$) factors into a term that is independent of the input/output label ($\transweightfun$)
and a term that is independent of the source state ($\emitweightfun$).

\end{frame}

\begin{frame}{}

{\em Universality:}
For any weak-normal form transducer $T$ there exists an equivalent in strict-normal form which can be found by applying the state-marginalization algorithm to eliminate null states.
For any transducer, there is an equivalent letter transducer in weak normal form, and therefore, in strict normal form.
\end{frame}

\begin{frame}{}

{\em Mealy machines} are transducers with I/O occurring on transitions, as with our general definition of the letter transducer.

{\em Moore machines} are transducers whose I/O is associated with states, as with our normal form.

\end{frame}

\begin{frame}{}

{\em Transducer composition:}
Given letter transducers
 $T = (\Omega_X, \Omega_Y, \States, \startstate, \laststate, \Transitions, \weight)$ and
 $U = (\Omega_Y, \Omega_Z, \States', \startstate', \laststate', \Transitions', \weight')$,
there exists a letter transducer $T\compose U = (\Omega_X, \Omega_Z, \States'' \ldots \weight'')$ 
such that $\forall x \in \Omega_X^\ast, z \in \Omega_Z^\ast$:
\[
\wtrans{\weight''}{x}{T\compose U}{z} = \sum_{y\in\Omega_Y^\ast} \wtrans{\weight}{x}{T}{y} \wtrans{\weight'}{y}{U}{z}
\]

\end{frame}

\begin{frame}{}

{\em Example construction of $TU$:}
Assume without loss of generality that $T$ and $U$ are in strict normal form.
Then $\States'' \subset \States \times \States'$,
$\startstate''=(\startstate,\startstate')$, $\laststate''=(\laststate,\laststate')$
and
\small
\begin{eqnarray*}
\lefteqn{\weight''((t,u),\omega_x,\omega_z,(t',u')) =} & & \\
& & \left\{ \begin{array}{ll}
\delta(t=t') \delta(\omega_x=\epsilon) \weight'(u,\epsilon,\omega_z,u') & \mbox{\small if $\statetype(u) \neq W$} \\
\weight(t,\omega_x,\epsilon,t') \delta(\omega_z=\epsilon) \delta(u=u') & \mbox{if $\statetype(u) = W,\statetype(t') \notin \{M,I\}$} \\
\displaystyle
\sum_{\omega_y \in \Omega_Y} \weight(t,\omega_x,\omega_y,t') \weight'(u,\omega_y,\omega_z,u') & \mbox{if $\statetype(u) = W,\statetype(t') \in \{M,I\}$} \\
0 & \mbox{otherwise}
\end{array} \right.
\end{eqnarray*}
\normalsize
The resulting transducer is in weak-normal form (it can be converted to a strict-normal form transducer by eliminating null states).

\end{frame}

\begin{frame}{}

{\em Transducer intersection:}
Given letter transducers
 $T = (\Omega_X, \Omega_T, \States, \startstate, \laststate, \Transitions, \weight)$ and
 $U = (\Omega_X, \Omega_U, \States', \startstate', \laststate', \Transitions', \weight')$,
there exists a letter transducer $T\fork U = (\Omega_X, \Omega_V, \States'' \ldots \weight'')$
where $\Omega_V \subseteq \gappedpair{T}{U}$
such that $\forall x \in \Omega_X^\ast, t \in \Omega_T^\ast, u \in \Omega_U^\ast$:
\[
\wtrans{\weight}{x}{T}{t} \wtrans{\weight'}{x}{U}{u} = \wtrans{\weight''}{x}{T\fork U}{(t,u)}
\]
where the term on the right is defined as follows
\[
\wtrans{\weight''}{x}{T\fork U}{(t,u)} = \sum_{v \in \Omega_V^\ast, S_1(v)=t, S_2(v)=u } \wtrans{\weight''}{x}{T\fork U}{v}
\]
Here $\Omega_V$ is the set of all possible pairwise alignment columns,
$v \in \Omega_V^\ast$ is a pairwise alignment and
$S_1(v)$ and $S_2(v)$ are the sequences in (respectively) the first and second rows of $v$.

\end{frame}

\begin{frame}{}

{\em Example construction of $T \fork U$:}
Assume without loss of generality that $T$ and $U$ are in strict normal form.
Then $\States'' \subset \States \times \States'$,
$\startstate''=(\startstate,\startstate')$, $\laststate''=(\laststate,\laststate')$
and
\small
\begin{eqnarray*}
\lefteqn{\weight''((t,u),\omega_x,(\omega_y,\omega_z),(t',u')) =} & & \\
 & & \left\{ \begin{array}{ll}
\delta(t=t') \delta(\omega_x=\omega_y=\epsilon) \weight'(u,\epsilon,\omega_z,u') & \mbox{if $\statetype(u) \neq W$} \\
\weight(t,\epsilon,\omega_x,t') \delta(\omega_x=\omega_z=\epsilon) \delta(u=u') & \mbox{if $\statetype(u) = W,\statetype(t) \neq W$} \\
\weight(t,\omega_x,\omega_y,t') \weight'(u,\omega_x,\omega_z,u') & \mbox{if $\statetype(t) = \statetype(u) = W$} \\
0 & \mbox{otherwise}
\end{array} \right.
\end{eqnarray*}
\normalsize
The resulting transducer is in weak-normal form (it can be converted to a strict-normal form transducer by eliminating null states).

\end{frame}

\begin{frame}{}

{\em Identity:}
There exists a transducer $\identity=(\Omega,\Omega\ldots)$ that copies its input identically to its output.
An example construction (not in normal form) is
\begin{eqnarray*}
\identity & = & (\Omega,\Omega,\{\phi\},\phi,\phi,\transitionsof{\identity},1) \\
\transitionsof{\identity} & = & \left\{(\phi,\omega,\omega,\phi):\omega\in\Omega\right\}
\end{eqnarray*}

\end{frame}

\begin{frame}{}

{\em Bifurcation:}
There exists a transducer $\idfork=(\Omega,\Omega^2\ldots)$ that duplicates its input in parallel.
That is, for input $x_1 x_2 x_3 \ldots$ it gives output $\dup{x_1}\dup{x_2}\dup{x_3}\ldots$.
An example construction (not in normal form) is
\begin{eqnarray*}
\idfork & = & (\Omega,\Omega^2,\{\phi\},\phi,\phi,\transitionsof{\idfork},1) \\
\transitionsof{\idfork} & = & \left\{\left(\phi,\omega,\dup{\omega},\phi\right):\omega\in\Omega\right\}
\end{eqnarray*}
It can be seen that $\idfork \transequiv \identity \fork \identity$.

An intersection $T \fork U$ may be considered a parallel composition of $\idfork$ with $T$ and $U$.
We write this as  $\forkfun{T}{U}$ or, diagrammatically,
\begin{parsetree}
 ( .. ( .$\idfork$. ( .$T$. ~ ) ( .$U$. ~ ) ) )
\end{parsetree}

We use the notation $\forkfun{T}{U}$ in several places, when it is convenient to have a placeholder transducer $\idfork$ at a bifurcating node in a tree.

\end{frame}

\begin{frame}{}

{\em Recognition profiles:}
A transducer $T$ is a {\em recognizer} if it has a null output alphabet, and so generates no output except the empty string.

\end{frame}

\begin{frame}{}

{\em Exact-match recognizer:}
For $S \in \Omega^\ast$, there exists a transducer $\recognize(S) = (\Omega,\emptyset\ldots \weight)$
that accepts the specific sequence $S$ with weight one, but rejects all other input sequences
\[
\wtrans{\weight}{x}{\recognize(S)}{\epsilon} = \delta(x=S)
\]
Note that $\recognize(S)$ has a null output alphabet, so its only possible output is the empty string, and it is a recognizer.

In general, if $T=(\Omega_X,\Omega_Y \ldots \weight')$ is any transducer then $\forall x \in \Omega_X^\ast, y \in \Omega_Y^\ast$
\[
\wtrans{\weight'}{x}{T}{y} \equiv \wtrans{\weight}{x}{T \compose \recognize(y)}{\epsilon}
\]

\end{frame}

\begin{frame}{}

An example construction (not in normal form) is
\begin{eqnarray*}
\recognize(S) & = & (\Omega,\emptyset,\mathbb{Z}_{\seqlen{S}+1},0,\seqlen{S},\transitionsof{\recognize},1) \\
\transitionsof{\recognize} & = & \left\{\left(n,\charat{S}{n+1},\epsilon,n+1\right):n \in \mathbb{Z}_{\seqlen{S}}\right)\}
\end{eqnarray*}
where $\mathbb{Z}_N$ is the set of integers modulo $N$,
 and $\charat{S}{k}$ is the $k$'th position of $S$ (for $1 \leq k \leq \seqlen{S}$).
Note that this construction has $\seqlen{S}+1$ states.

\end{frame}

\begin{frame}{}

{\em Generative transducers:}
A transducer $T$ is generative (or ``a {\em generator}'') if it has a null input alphabet, and so rejects any input except the empty string.
Then $T$ may be regarded as a state machine that generates an output, equivalent to a Hidden Markov Model.
Define the probability (weight) distribution over the output sequence
\[
P(x|T) \equiv \wtrans{\weight}{\epsilon}{T}{x}
\]

\end{frame}

\begin{frame}{}

{\em Chapman-Kolmogorov equation:}

If $T_t$ is a transducer parameterized by a continuous time parameter $t$,
modeling the evolution of a sequence for time $t$ under a continuous-time Markov process,
then the Chapman-Kolmogorov equation can be expressed as a transducer equivalence
\[
T_t \compose T_u \transequiv T_{t+u}
\]

The TKF91 transducers, for example, have this property.
Furthermore, for TKF91, $T_{t+u}$ has the same state and transition structure as $T_t$,
so this is a kind of self-similarity.

\end{frame}

\begin{frame}{}

We can write the instantaneous form of Chapman-Kolmogorov in transducer form,
\[
\frac{d}{dt} p_t = p_t Q
\]
where $p_t$ is the generator transducer for the sequence at time $t$
and $Q$ is an instantaneous rate transducer, a generalized transducer
whose weight is negative for on-diagonal elements (i.e. when the inputs and outputs are equal):
\[
Q = Q_{+} - (\identity \circ \projection(Q_{+}))
\]
where $Q_{+}$ is a positive (i.e. nonnegatively-weighted) transducer representing the rates
and $\projection(Q_{+})$ is the projection of $Q_{+}$ to a recognizer
(marginalizing the output tape).

This further implies a transducer matrix exponential
\[
T_t = \exp(Qt)
\]

\end{frame}

\begin{frame}{}
Felsenstein pruning recursion for transducers
\[
F_n = \left\{
\begin{array}{ll}
\left( M^{(l)} F_l \right) \fork \left( M^{(r)} F_r \right) & \mbox{if $n$ internal} \\
\nabla(y_n) & \mbox{if $n$ leaf}
\end{array}
\right.
\]
The Felsenstein likelihood is given by summing all paths through $R F_{\mbox{root}}$,
where
\itemb
\item $R$ is generator for root distribution,
\item $M^{(n)}$ is transducer on branch above node $n$,
\item $y_n$ is sequence at leaf node $n$,
\item $AB$ is transducer composition (matrix multiplication),
\item $A \circ B$ is transducer intersection (pointwise product).
\iteme
\end{frame}


\section{Multiple alignment with the links model}

\begin{frame}{}

\itemb
\item Phylogenetic transducers/HMMs
 \inone{Exhaustive DP (c.f. Hein 2001)}
\item MCMC
 \itemb
 \item Gibbs-sampling kernels for local tree neighborhoods
 \item Chaining programs together via Metropolis-Hastings propose/accept/reject schemes
 \item Redelings and Suchard's kernel
 \item Bouchard-C\^{o}t\'{e}, Jordan, and Klein's kernel
 \iteme
\iteme

\end{frame}

\section{More realistic evolutionary models}

\begin{frame}{}

\itemb
\item The long indel model
 \itemb
 \item Knudsen-Miyamoto transducer
 \item \Miklos-Lunter-Holmes transducer
 \iteme
\item Short-range context-sensitive substitution models
 \itemb
 \item Siepel-Haussler approach: model $k$-mer as $\Omega^k$-state stochastic process $x_1(t), x_2(t) \ldots x_k(t)$
  \itemb
  \item Conditional probability of next aligned pair, given previous $k-1$ aligned pairs:
$P(x_k(0),x_k(t)|x_1(0) \ldots x_{k-1}(0),x_1(t) \ldots x_{k-1}(t))$
  \iteme
 \item Lunter-Hein approach: model full $\Omega^L$ process.
Rate matrix is sum of $k$-mer terms, some of which commute, some don't.
Taylor series for matrix exponential can be factorised and leading terms computed by DP.
 \item Short-range context-dependence: relevant for protein?
Probably not for substitution (eg recent Brenner {\em et al} study)
but maybe for indels (which might look like local duplications)
 \iteme
\iteme
\end{frame}

\begin{frame}{}
\itemb
\item Short-range context-sensitive indel models
 \inone{Motivation: many indels appear, empirically, to be miniature local duplications}
\item Mutation-selection models
 \inone{Description of model}
\item Rearrangement models: combinatorial explosion in histories, MCMC slow
 \itemb
 \item Most authors write the genome in bigger units (e.g. identifiable genes or blocks of synteny, rather than individual nucleotides)
 \item Hannenhalli \& Pevzner, 1999; Siepel, 2002; \Miklos, Bioinformatics 2003
 \iteme
\iteme

\end{frame}




\section*{Summary}

\begin{frame}{Summary}

  % Keep the summary *very short*.
  \begin{itemize}
  \item Indels
  \end{itemize}

\end{frame}


\end{document}
